{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-17T20:40:21.155130Z",
     "start_time": "2025-11-17T20:39:55.819764Z"
    }
   },
   "source": [
    "from  datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "\n",
    "X_train = train_df[\"text\"]\n",
    "y_train = train_df[\"label\"]\n",
    "\n",
    "X_test = test_df[\"text\"]\n",
    "y_test = test_df[\"label\"]\n",
    "train_df.head()\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\PycharmProjects\\Nlp-final-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T20:41:12.827681Z",
     "start_time": "2025-11-17T20:40:38.270647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,\n",
    "    ngram_range=(1,2),\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(max_iter=500)\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "logreg_pred = logreg.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logreg_pred))\n",
    "print(classification_report(y_test, logreg_pred))\n",
    "\n"
   ],
   "id": "56e76c4fc2dacdce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.88264\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88     12500\n",
      "           1       0.88      0.88      0.88     12500\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T20:42:59.683362Z",
     "start_time": "2025-11-17T20:42:58.743144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "\n",
    "svm_pred = svm.predict(X_test_tfidf)\n",
    "\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
    "print(classification_report(y_test, svm_pred))\n"
   ],
   "id": "e5ce83ce98c92782",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.8702\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87     12500\n",
      "           1       0.88      0.86      0.87     12500\n",
      "\n",
      "    accuracy                           0.87     25000\n",
      "   macro avg       0.87      0.87      0.87     25000\n",
      "weighted avg       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T20:43:24.138852Z",
     "start_time": "2025-11-17T20:43:12.384722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\n",
    "    \"distilbert-base-uncased\"\n",
    ")\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    batch_size=512\n",
    ")\n",
    "\n",
    "dataset_tokenized = dataset_tokenized.remove_columns([\"text\"])\n",
    "dataset_tokenized = dataset_tokenized.rename_column(\"label\", \"labels\")\n",
    "dataset_tokenized.set_format(\"torch\")\n",
    "\n"
   ],
   "id": "69e6ccd1b2f8229f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T20:47:45.216315Z",
     "start_time": "2025-11-17T20:47:45.074087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DistilBertForSequenceClassification,AutoTokenizer,HFAutoTrainer,HFFineTuningConfig\n",
    "\n",
    "\n",
    "# Model & tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# CONFIG equivalent to TrainingArguments\n",
    "config = HFFineTuningConfig(\n",
    "    output_dir=\"./distilbert_imdb\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = HFAutoTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n"
   ],
   "id": "8d240cb734f9e1e0",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HFAutoTrainer' from 'transformers' (C:\\Users\\Administrator\\PycharmProjects\\Nlp-final-project\\.venv\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mtransformers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DistilBertForSequenceClassification,AutoTokenizer,HFAutoTrainer,HFFineTuningConfig\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# Model & tokenizer\u001B[39;00m\n\u001B[32m      5\u001B[39m model = DistilBertForSequenceClassification.from_pretrained(\n\u001B[32m      6\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mdistilbert-base-uncased\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m      7\u001B[39m     num_labels=\u001B[32m2\u001B[39m\n\u001B[32m      8\u001B[39m )\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'HFAutoTrainer' from 'transformers' (C:\\Users\\Administrator\\PycharmProjects\\Nlp-final-project\\.venv\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilbert_imdb\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"test\"]\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ],
   "id": "deff175dee3e1b21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = trainer.predict(dataset_tokenized[\"test\"])\n",
    "distilbert_preds = predictions.predictions.argmax(-1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(\"DistilBERT Accuracy:\", accuracy_score(y_test, distilbert_preds))\n",
    "print(classification_report(y_test, distilbert_preds))\n"
   ],
   "id": "3cb9f74ae8f83cfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9eff008512c356b8",
   "metadata": {},
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()  # Load variables from .env file\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "movie_title = \"Top Gun: Maverick\"\n",
    "\n",
    "# Search for movie ID\n",
    "search_url = f\"https://api.themoviedb.org/3/search/movie?api_key={api_key}&query={movie_title}\"\n",
    "response = requests.get(search_url).json()\n",
    "movie_id = response[\"results\"][0][\"id\"]\n",
    "\n",
    "# Get cast and crew\n",
    "credits_url = f\"https://api.themoviedb.org/3/movie/{movie_id}/credits?api_key={api_key}\"\n",
    "credits = requests.get(credits_url).json()\n",
    "\n",
    "# Extract names\n",
    "actors = [member[\"name\"] for member in credits[\"cast\"][:5]]\n",
    "directors = [member[\"name\"] for member in credits[\"crew\"] if member[\"job\"] == \"Director\"]\n",
    "\n",
    "print(\"Actors:\", actors)\n",
    "print(\"Director:\", directors)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "970ef3cf",
   "metadata": {},
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sample_review = train_data[0][\"text\"]\n",
    "doc = nlp(sample_review)\n",
    "\n",
    "# Extract named entities\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in [\"PERSON\"]]\n",
    "print(\"Named Entities:\", entities)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    X_train.tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256\n",
    ")\n",
    "\n",
    "test_encodings = tokenizer(\n",
    "    X_test.tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256\n",
    ")"
   ],
   "id": "1c1da1b610faecce",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

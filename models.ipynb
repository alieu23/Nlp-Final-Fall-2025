{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-18T15:51:55.360085Z",
     "start_time": "2025-11-18T15:51:19.474726Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/imdb\")\n",
    "\n",
    "train_df = pd.DataFrame(dataset[\"train\"])\n",
    "test_df = pd.DataFrame(dataset[\"test\"])\n",
    "\n",
    "train_df.head()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\PycharmProjects\\Nlp-final-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T16:18:25.957994Z",
     "start_time": "2025-11-18T16:05:24.446030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "#When on CPU, slow, takes low computing power, lwss accurate\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "#When using GCP, for faster NER and accuracy\n",
    "#nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "def extract_person_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "\n",
    "# Apply to dataset\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "\n",
    "#Use,if on GCP\n",
    "#train_df[\"persons\"] = train_df[\"text\"].progress_apply(extract_person_entities)\n",
    "#test_df[\"persons\"] = test_df[\"text\"].progress_apply(extract_person_entities)\n",
    "\n",
    "#Else, when on CPU\n",
    "persons_train = []\n",
    "for doc in nlp.pipe(train_df[\"text\"], batch_size=32, n_process=4):\n",
    "    ents = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    persons_train.append(ents)\n",
    "\n",
    "train_df[\"persons\"] = persons_train\n",
    "train_df.head()\n",
    "\n",
    "# persons_test = []\n",
    "# for doc in nlp.pipe(test_df[\"text\"], batch_size=32, n_process=4):\n",
    "#     ents = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "#     persons_test.append(ents)\n",
    "#\n",
    "# test_df[\"persons\"] = persons_test\n"
   ],
   "id": "77ed392d282d3e59",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                text  label  \\\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0   \n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0   \n",
       "2  If only to avoid making this type of film in t...      0   \n",
       "3  This film was probably inspired by Godard's Ma...      0   \n",
       "4  Oh, brother...after hearing about this ridicul...      0   \n",
       "\n",
       "                                     persons  \n",
       "0          [Lena, Ingmar Bergman, John Ford]  \n",
       "1  [Vincent Gallo's, johnson, Chloe Sevigny]  \n",
       "2                                         []  \n",
       "3               [Godard, Lena Nyman, Godard]  \n",
       "4                                [Peggy Lee]  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>persons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Lena, Ingmar Bergman, John Ford]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Vincent Gallo's, johnson, Chloe Sevigny]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Godard, Lena Nyman, Godard]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Peggy Lee]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=20_000,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n"
   ],
   "id": "86b784583c98f19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=3000, n_jobs=-1)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "pred_lr = log_reg.predict(X_test)\n",
    "\n",
    "print(\"=== Logistic Regression Classification Report ===\")\n",
    "print(classification_report(y_test, pred_lr))\n",
    "print(\"Macro F1:\", f1_score(y_test, pred_lr, average=\"macro\"))\n"
   ],
   "id": "d1b3369895a7ad3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "pred_svm = svm.predict(X_test)\n",
    "\n",
    "print(\"=== Linear SVM Classification Report ===\")\n",
    "print(classification_report(y_test, pred_svm))\n",
    "print(\"Macro F1:\", f1_score(y_test, pred_svm, average=\"macro\"))\n"
   ],
   "id": "f6157d564f039d11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def compute_entity_skew(df, predictions):\n",
    "    entity_sentiments = defaultdict(list)\n",
    "\n",
    "    for persons, sentiment in zip(df[\"persons\"], predictions):\n",
    "        for p in persons:\n",
    "            entity_sentiments[p].append(sentiment)\n",
    "\n",
    "    entity_skew = []\n",
    "    for person, sentiments in entity_sentiments.items():\n",
    "        avg_sentiment = np.mean(sentiments)\n",
    "        count = len(sentiments)\n",
    "        entity_skew.append((person, avg_sentiment, count))\n",
    "\n",
    "    skew_df = pd.DataFrame(entity_skew, columns=[\"entity\", \"avg_sentiment\", \"count\"])\n",
    "    return skew_df.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "# Example using Logistic Regression predictions\n",
    "skew_df = compute_entity_skew(test_df, pred_lr)\n",
    "skew_df.head(10)\n"
   ],
   "id": "d5a882a64cfef55b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "biased_positive = skew_df[skew_df[\"avg_sentiment\"] > 0.7].head(10)\n",
    "biased_negative = skew_df[skew_df[\"avg_sentiment\"] < 0.3].head(10)\n",
    "\n",
    "print(\"Entities with unusually positive sentiment:\")\n",
    "print(biased_positive)\n",
    "\n",
    "print(\"\\nEntities with unusually negative sentiment:\")\n",
    "print(biased_negative)\n"
   ],
   "id": "f5bb3418e03081dc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
